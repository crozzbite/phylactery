import logging
import os
import json
import time
from pathlib import Path
from typing import Annotated, TYPE_CHECKING, cast
from langgraph.graph.message import add_messages

from dotenv import load_dotenv
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_core.runnables import Runnable
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph
from typing_extensions import TypedDict

from .models import Agent

if TYPE_CHECKING:
    from langgraph.graph.graph import CompiledGraph
    from .mcp_client import MCPClient

load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# Define State
class AgentState(TypedDict):
    """The state of the agent graph."""

    messages: Annotated[list[BaseMessage], add_messages]
    intent: str  # "conversation" or "task"
    task: str  # Phase 6: Original user request for finalizer context
    plan: list[str]  # The high-level plan generated by the planner
    past_steps: list[tuple[str, str]]  # (step_description, result) for completed steps
    current_step: int  # Current index in the plan
    iteration_count: int  # To prevent infinite loops


class AgentEngine:
    """The core engine that runs agents via LangGraph."""

    llm: BaseChatModel | Runnable[object, AIMessage]
    history: list[BaseMessage]
    last_used: float

    def __init__(self, agent: Agent) -> None:
        self.agent = agent
        self.history = []
        self.cwd = os.getcwd()
        self.last_used = time.time()
        self.tools: list[dict[str, object]] = []
        self.mcp_clients: list["MCPClient"] = []
        self.tool_to_client: dict[str, "MCPClient"] = {}

        # Use agent-specific provider if set, otherwise use global env
        env_provider = os.getenv("AI_PROVIDER", "ollama") or "ollama"
        raw_provider = agent.ai_provider or env_provider
        provider = raw_provider.lower()

        logger.info(f"Initializing engine for agent: {agent.role} using provider: {provider}")

        # Basic LLM initialization
        llm: BaseChatModel
        if provider == "ollama":
            model_name = os.getenv("OLLAMA_MODEL", "llama3")
            logger.info(f"Using Ollama model: {model_name}")
            llm = ChatOllama(model=model_name, temperature=0.1)
        elif provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEY not found and provider is 'openai'")
            model_name = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
            logger.info(f"Using OpenAI model: {model_name}")
            llm = ChatOpenAI(model=model_name, api_key=api_key, temperature=0.2)  # type: ignore[arg-type]
        else:
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key:
                raise ValueError("GEMINI_API_KEY not found and provider is 'gemini'")
            logger.info("Using Gemini model: gemini-1.5-flash")
            llm = ChatGoogleGenerativeAI(
                model="gemini-1.5-flash", google_api_key=api_key, temperature=0.7
            )

        # Dual LLM Architecture (Phase 6)
        self.llm_text = llm  # For Router, Planner, Finalizer (NO TOOLS)
        self.llm_exec = llm  # For Executor (WITH TOOLS, bound later)
        self.llm = llm  # Backward compatibility
        self.graph = self._build_graph()

    def _sanitize_history(self, messages: list[BaseMessage]) -> list[BaseMessage]:
        """
        Sanitize history to fix OpenAI 400 errors:
        1. Remove orphaned ToolMessages (no preceding AIMessage with tool_calls)
        2. Ensure AIMessage with tool_calls is followed by ToolMessage
        """
        clean_messages = []
        for i, msg in enumerate(messages):
            if isinstance(msg, ToolMessage):
                # Check if previous message exists and has tool_calls
                if i > 0 and isinstance(messages[i-1], AIMessage) and messages[i-1].tool_calls:
                    # Also check if tool_call_id matches (optional but good)
                    # For now just keep it if parent exists
                    clean_messages.append(msg)
                else:
                    logger.warning(f"üßπ Removing orphaned ToolMessage: {msg.content[:50]}...")
            elif isinstance(msg, AIMessage) and msg.tool_calls:
                # Check if next message is ToolMessage (or if we are waiting for one)
                # If this is the LAST message, we might be waiting for tool execution, so keep it.
                # If not last, and next is NOT tool message, then it's an orphan call?
                # Actually, OpenAI requires tool_calls to be followed by tool messages.
                # If we have [AIMessage(calls), HumanMessage], that's invalid.
                
                # Simple logic for now: Keep AIMessage, but if we filter out its ToolMessage later, 
                # we might have an issue. Start with just filtering orphans.
                clean_messages.append(msg)
            else:
                clean_messages.append(msg)
        return clean_messages


    async def _init_mcp_tools(self, servers: list[str]) -> None:
        """Initialize MCP clients and fetch tools."""
        from mcp import StdioServerParameters
        from .mcp_client import MCPClient

        # Phase 6 Fix: Multi-server configuration
        SERVER_CONFIG = {
            "filesystem": {
                "command": "npx",
                "args_template": ["-y", "@modelcontextprotocol/server-filesystem", "<ROOT>"],
                "needs_root": True
            },
            "email": {
                "command": "npx",
                "args_template": ["-y", "@modelcontextprotocol/server-email"],
                "needs_root": False
            },
            "gmail": {
                "command": "npx",
                "args_template": ["-y", "@modelcontextprotocol/server-gmail"],
                "needs_root": False
            }
        }

        for server_name in servers:
            config = SERVER_CONFIG.get(server_name)
            if not config:
                logger.warning(f"‚ö†Ô∏è Unknown MCP server: {server_name}, skipping...")
                continue
            
            # Prepare args
            args = config["args_template"].copy()
            if config.get("needs_root"):
                # Determine project root for filesystem
                project_root = str(Path(os.getcwd()).parents[3] / "Desktop" / "IA-proyects")
                if not os.path.exists(project_root):
                    project_root = os.getcwd()
                # Replace <ROOT> placeholder
                args = [arg.replace("<ROOT>", project_root) for arg in args]
            
            params = StdioServerParameters(
                command=config["command"],
                args=args,
            )
            client = MCPClient(params)
            try:
                await client.connect()
                server_tools = await client.list_tools()
                self.mcp_clients.append(client)
                for t in server_tools:
                    # Phase 6 Fix: Namespace tools to avoid collisions
                    tool_name = f"{server_name}.{t['name']}"
                    tool_dict = cast(dict[str, object], t)
                    tool_dict["name"] = tool_name  # Rename for uniqueness
                    self.tools.append(tool_dict)
                    self.tool_to_client[tool_name] = client
                logger.info(f"‚úÖ Connected to MCP Server: {server_name} ({len(server_tools)} tools)")
            except Exception as e:
                logger.error(f"‚ùå Failed to connect to MCP Server {server_name}: {e}")

        if self.tools and isinstance(self.llm_exec, BaseChatModel):
            logger.info(f"üõ†Ô∏è Binding {len(self.tools)} tools to Executor LLM.")
            self.llm_exec = self.llm_exec.bind_tools(self.tools)

    def _build_graph(self) -> "CompiledGraph":
        workflow = StateGraph(AgentState)

        # --- NODE: ROUTER (Conversational Entry Point) ---
        async def router(state: AgentState) -> dict[str, object]:
            """Conversational LLM that decides: answer directly or delegate to planner."""
            logger.info("üß† [ROUTER] Starting intent classification...")
            messages = state["messages"]
            last_human_msg = next((m.content for m in reversed(messages) if isinstance(m, HumanMessage)), "")

            # RAG Retrieval for context
            from .memory import memory
            relevant_docs = await memory.retrieve_relevant(str(last_human_msg), k=2)
            
            skills_context = ""
            if relevant_docs:
                skills_context = "\n\n### AVAILABLE KNOWLEDGE\n"
                for doc in relevant_docs:
                    skills_context += f"- {doc.metadata.get('name')}: {doc.page_content[:200]}...\n"

            router_prompt = (
                f"You are Phylactery, a conversational AI assistant for SkullRender.\n"
                f"Role: {self.agent.role}\n"
                f"Instructions: {self.agent.instructions}\n"
                f"{skills_context}\n\n"
                "### YOUR TASK\n"
                "Analyze the user's message and classify the intent:\n\n"
                "**TASK** - If the user wants you to DO something (create, write, modify, delete, run, execute, build, generate, etc.)\n"
                "**CONVERSATION** - If the user is asking a question, wants information, or is just chatting\n\n"
                "### CRITICAL RULES\n"
                "- Action verbs (crear, hacer, escribir, generar, ejecutar, etc.) = TASK\n"
                "- Questions (c√≥mo, qu√©, por qu√©, etc.) = CONVERSATION\n"
                "- If unsure, default to TASK (better to try than refuse)\n\n"
                "### RESPONSE FORMAT (STRICT JSON)\n"
                "Return ONLY this JSON structure:\n"
                "{\"intent\": \"task\", \"task_description\": \"what to do\"}\n"
                "OR\n"
                "{\"intent\": \"conversation\", \"response\": \"your answer\"}\n\n"
                "### EXAMPLES\n"
                "User: '¬øC√≥mo hago un PR?' ‚Üí {\"intent\": \"conversation\", \"response\": \"Para hacer un PR...\"}\n"
                "User: 'Crea un archivo test.txt' ‚Üí {\"intent\": \"task\", \"task_description\": \"Create file test.txt\"}\n"
                "User: 'Haz una p√°gina web' ‚Üí {\"intent\": \"task\", \"task_description\": \"Build a web page\"}\n"
                "User: '¬øQu√© es Phylactery?' ‚Üí {\"intent\": \"conversation\", \"response\": \"Phylactery es...\"}\n"
            )

            response = await self.llm_text.ainvoke([SystemMessage(content=router_prompt)] + messages[-5:])
            logger.info(f"üß† Router raw response: {response.content[:200]}...")
            
            try:
                content = str(response.content)
                import re
                json_match = re.search(r'(\{[\s\S]*?"intent"[\s\S]*?\})', content)
                if json_match:
                    data = json.loads(json_match.group(1))
                    intent = data.get("intent", "conversation")
                    logger.info(f"üéØ Router classified intent: {intent}")
                    
                    if intent == "conversation":
                        # Direct response
                        answer = data.get("response", content)
                        return {
                            "messages": [AIMessage(content=answer)],
                            "intent": "conversation",
                            "task": "",  # Phase 6
                            "plan": [],
                            "past_steps": [],
                            "current_step": 0,
                            "iteration_count": 0
                        }
                    else:
                        # Delegate to planner
                        task_desc = data.get("task_description", str(last_human_msg))
                        logger.info(f"üöÄ Delegating to Planner: {task_desc}")
                        return {
                            "intent": "task",
                            "task": task_desc,  # Phase 6: Save original task
                            "messages": [HumanMessage(content=task_desc)],
                            "plan": [],
                            "past_steps": [],
                            "current_step": 0,
                            "iteration_count": 0
                        }
            except Exception as e:
                logger.error(f"Router failed to parse JSON: {e}")
            
            # Fallback: Check for action verbs, default to task if found
            action_verbs = ["crea", "crear", "haz", "hacer", "escribe", "genera", "ejecuta", "construye", "build", "create", "write", "make", "generate"]
            if any(verb in str(last_human_msg).lower() for verb in action_verbs):
                logger.warning(f"‚ö†Ô∏è Router fallback: Detected action verb, treating as TASK")
                return {
                    "intent": "task",
                    "task": str(last_human_msg),  # Phase 6
                    "messages": [HumanMessage(content=str(last_human_msg))],
                    "plan": [],
                    "past_steps": [],
                    "current_step": 0,
                    "iteration_count": 0
                }
            
            # Final fallback: treat as conversation
            logger.warning(f"‚ö†Ô∏è Router fallback: Treating as CONVERSATION")
            return {
                "messages": [AIMessage(content=str(response.content))],
                "intent": "conversation",
                "plan": [],
                "past_steps": [],
                "current_step": 0,
                "iteration_count": 0
            }

        # --- NODE: PLANNER ---
        async def planner(state: AgentState) -> dict[str, object]:
            """Generates a step-by-step plan for the user request."""
            logger.info("üìã [PLANNER] Generating execution plan...")
            messages = state["messages"]
            past_steps = state.get("past_steps", [])
            last_human_msg = next((m.content for m in reversed(messages) if isinstance(m, HumanMessage)), "")

            # Context from past steps
            past_context = ""
            if past_steps:
                past_context = "\n\n### COMPLETED STEPS\n"
                for step_desc, result in past_steps:
                    past_context += f"- {step_desc}: {result[:100]}...\n"

            planner_prompt = (
                f"You are the PLANNER for Phylactery.\n"
                f"Task: {last_human_msg}\n"
                f"{past_context}\n"
                "### YOUR JOB\n"
                "Break down the task into a sequence of atomic steps.\n"
                "Each step should be a single, clear action.\n\n"
                "### RESPONSE FORMAT\n"
                "Return JSON: {\"plan\": [\"step1\", \"step2\", ...]}\n"
                "Example: {\"plan\": [\"Create directory 'test'\", \"Write file 'hello.txt' with content 'Hello World'\", \"List directory contents\"]}"
            )

            # Pass actual user message for better context (Phase 6 Fix)
            user_msg = HumanMessage(content=last_human_msg)
            response = await self.llm_text.ainvoke([SystemMessage(content=planner_prompt), user_msg])
            
            try:
                content = str(response.content)
                import re
                json_match = re.search(r'(\{[\s\S]*?"plan"[\s\S]*?\]\})', content)
                if json_match:
                    data = json.loads(json_match.group(1))
                    return {"plan": data["plan"], "current_step": 0}
            except Exception as e:
                logger.error(f"Planner failed to generate JSON: {e}")
            
            return {"plan": [str(last_human_msg)], "current_step": 0}

        # --- NODE: EXECUTOR (Agent) ---
        async def executor(state: AgentState) -> dict[str, object]:
            """Executes the current step of the plan."""
            messages = state["messages"]
            plan = state.get("plan", [])
            step_idx = state.get("current_step", 0)
            logger.info(f"ü¶¥ [EXECUTOR] Executing step {step_idx + 1}/{len(plan)}: {plan[step_idx] if step_idx < len(plan) else 'N/A'}")
            
            current_task = plan[step_idx] if step_idx < len(plan) else "Complete the request."

            # Mandatory Rules
            mandatory_rules = (
                "\n\n### MANDATORY EXECUTION RULES\n"
                "1. **DEFAULT PATH**: If no path is specified, use `phylactery-app/`.\n"
                "2. **.ENV PROTECTION**: READ-ONLY access to .env files. NEVER modify them.\n"
                f"3. **CURRENT TASK**: {current_task}\n"
                "4. **PRECISION**: Execute ONLY the current task. Don't wander."
            )

            system_prompt = SystemMessage(
                content=f"Role: Executor\nScope: {self.agent.role}\n{mandatory_rules}"
            )

            # Tools help for executor
            tools_help = ""
            if self.tools:
                tools_help = "\n\n### AVAILABLE TOOLS\n" + "\n".join([f"- {t['name']}: {t['description']}" for t in self.tools])
                system_prompt.content += tools_help

            # Phase 6 Fix: Detect if last message is ToolMessage (post-tool contract)
            is_post_tool = len(messages) > 0 and isinstance(messages[-1], ToolMessage)
            
            if is_post_tool:
                # Force summary instead of allowing more tool calls
                system_prompt.content += (
                    "\n\n### POST-TOOL INSTRUCTION\n"
                    "Tool output has been received above. DO NOT call more tools.\n"
                    "Summarize the result and explain what changed. Mark the step as complete."
                )
                # Phase 6 CRITICAL FIX: Use llm_text (no tools) to prevent loops
                response = await self.llm_text.ainvoke([system_prompt] + messages)
            else:
                # Normal execution with tools
                response = await self.llm_exec.ainvoke([system_prompt] + messages)
            
            logger.info(f"Executor responded for step {step_idx}")

            # Manual JSON Parsing for tools (some models need it)
            if isinstance(response, AIMessage) and not response.tool_calls:
                import re
                json_match = re.search(r'(\{[\s\S]*?"name"[\s\S]*?"parameters"[\s\S]*?\})', str(response.content))
                if json_match:
                    try:
                        data = json.loads(json_match.group(1).strip())
                        tool_call = {"name": data["name"], "args": data["parameters"], "id": f"call_{int(time.time())}", "type": "tool_call"}
                        response = AIMessage(content=response.content, tool_calls=[tool_call])
                    except: pass

            return {"messages": [response], "iteration_count": state.get("iteration_count", 0) + 1}

        # --- NODE: TOOLS ---
        async def call_tools(state: AgentState) -> dict[str, list[BaseMessage]]:
            """Execute tool calls from the last message."""
            logger.info("üîß [TOOLS] Calling MCP tools...")
            last_message = state["messages"][-1]
            tool_messages: list[BaseMessage] = []

            if hasattr(last_message, "tool_calls") and last_message.tool_calls:
                for tool_call in last_message.tool_calls:
                    tool_name = tool_call["name"]
                    client = self.tool_to_client.get(tool_name)
                    if client:
                        try:
                            result = await client.call_tool(tool_name, tool_call["args"])
                            # Phase 6 Fix: Stringify non-string results
                            if not isinstance(result, str):
                                import json
                                result = json.dumps(result, ensure_ascii=False, indent=2)
                            tool_messages.append(ToolMessage(content=result, tool_call_id=tool_call["id"]))
                        except Exception as e:
                            tool_messages.append(ToolMessage(content=f"Error: {e}", tool_call_id=tool_call["id"]))
            return {"messages": tool_messages}

        def route_from_router(state: AgentState) -> str:
            """Routes based on intent classification."""
            intent = state.get("intent", "conversation")
            if intent == "task":
                return "planner"
            return END

        def reviewer(state: AgentState) -> str:
            """Decides whether to continue the plan, finalize, or end."""
            last_message = state["messages"][-1]
            if hasattr(last_message, "tool_calls") and last_message.tool_calls:
                return "tools"
            
            step_idx = state.get("current_step", 0)
            plan = state.get("plan", [])
            iteration_count = state.get("iteration_count", 0)
            intent = state.get("intent", "conversation")
            
            # Phase 6 Fix: Route to finalizer for task completion
            if intent == "task":
                # If plan is complete or hit iteration limit, finalize
                if step_idx + 1 >= len(plan) or iteration_count >= 15:
                    return "finalizer"
                # Otherwise continue to next step
                if step_idx + 1 < len(plan):
                    return "next_step"
            
            return END

        # --- NODE: FINALIZER (Phase 6) ---
        async def finalizer(state: AgentState) -> dict[str, object]:
            """Synthesizes natural language response from execution results."""
            logger.info("‚ú® [FINALIZER] Synthesizing final response...")
            past_steps = state.get("past_steps", [])
            task = state.get("task", "")  # Phase 6 Fix: Use original task
            iteration_count = state.get("iteration_count", 0)
            messages = state["messages"]
            
            # Build context from completed steps
            steps_summary = ""
            if past_steps:
                steps_summary = "\n### COMPLETED STEPS\n"
                for step_desc, result in past_steps:
                    steps_summary += f"- {step_desc}: {result[:150]}...\n"
            
            # Detect if stopped by safety limit
            stopped_early = iteration_count >= 15
            
            finalizer_prompt = (
                f"You are Phylactery's Finalizer. Your job is to translate execution results into natural language.\n\n"
                f"Original request: {task or 'Unknown'}\n"  # Phase 6 Fix
                f"{steps_summary}\n\n"
                f"### YOUR TASK\n"
            )
            
            if stopped_early:
                finalizer_prompt += (
                    "The execution was stopped due to safety limits (15 iterations).\n"
                    "Explain what was accomplished and what remains incomplete.\n"
                )
            else:
                finalizer_prompt += (
                    "Summarize what was accomplished in a friendly, conversational tone.\n"
                    "Be specific about files created, actions taken, and results.\n"
                )
            
            finalizer_prompt += (
                "\n### RESPONSE FORMAT\n"
                "Respond in natural language (Spanish or English). Be concise but informative.\n"
                "Example: 'He creado el archivo prueba5.txt en la carpeta phylactery-app con el contenido solicitado. ¬°Listo!'\n"
            )
            
            response = await self.llm_text.ainvoke([SystemMessage(content=finalizer_prompt)])
            logger.info(f"‚ú® Finalizer generated response: {str(response.content)[:100]}...")
            
            return {
                "messages": [AIMessage(content=response.content)],
                "intent": "conversation",  # Mark as complete
                "iteration_count": iteration_count + 1
            }

        def advance_step(state: AgentState) -> dict[str, object]:
            """Advances to next step and records the completed step."""
            current_idx = state.get("current_step", 0)
            plan = state.get("plan", [])
            past_steps = state.get("past_steps", [])
            
            # Record completed step
            if current_idx < len(plan):
                last_msg = state["messages"][-1] if state["messages"] else None
                result = str(last_msg.content).strip() if last_msg else ""
                
                # Phase 6 Fix: Fallback for empty results
                if not result:
                    for m in reversed(state["messages"]):
                        if isinstance(m, ToolMessage) and m.content:
                            result = str(m.content)[:500]
                            break
                if not result:
                    result = "Completed"
                
                past_steps.append((plan[current_idx], result))
            
            return {
                "current_step": current_idx + 1,
                "past_steps": past_steps
            }

        workflow.add_node("router", router)
        workflow.add_node("planner", planner)
        workflow.add_node("executor", executor)
        workflow.add_node("tools", call_tools)
        workflow.add_node("advance", advance_step)
        workflow.add_node("finalizer", finalizer)  # Phase 6

        workflow.set_entry_point("router")
        
        workflow.add_conditional_edges(
            "router",
            route_from_router,
            {
                "planner": "planner",
                END: END
            }
        )
        
        workflow.add_edge("planner", "executor")
        
        workflow.add_conditional_edges(
            "executor",
            reviewer,
            {
                "tools": "tools",
                "next_step": "advance",
                "finalizer": "finalizer",  # Phase 6
                END: END
            }
        )
        workflow.add_edge("tools", "executor")
        workflow.add_edge("advance", "executor")
        workflow.add_edge("finalizer", END)  # Phase 6: Finalizer always ends

        return workflow.compile()

    async def ainvoke(self, message: str) -> str:
        """Runs the graph with a single user message, maintaining history."""
        self.last_used = time.time()
        current_message = HumanMessage(content=message)
        self.history.append(current_message)

        # CRITICAL: Pass SANITIZED history to preserve AIMessage‚ÜíToolMessage relationships
        sanitized_history = self._sanitize_history(self.history)
        inputs = {
            "messages": sanitized_history,  # Phase 8 Fix: No orphaned tools!
            "intent": "",
            "plan": [],
            "past_steps": [],
            "current_step": 0,
            "iteration_count": 0
        }
        result = await self.graph.ainvoke(inputs)

        # Update history with new messages from the run
        new_msgs = [m for m in result["messages"] if m not in self.history]
        self.history.extend(new_msgs)

        ai_responses = [m for m in result["messages"] if isinstance(m, AIMessage)]
        if ai_responses:
            return str(ai_responses[-1].content)

        return "The spirits are silent."
