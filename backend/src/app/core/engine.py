import logging
import os
import json
import time
from pathlib import Path
from typing import Annotated, TYPE_CHECKING, cast, List
from langgraph.graph.message import add_messages

from dotenv import load_dotenv
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_core.runnables import Runnable
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph
import uuid
from typing_extensions import TypedDict

from .models import Agent

if TYPE_CHECKING:
    from langgraph.graph.graph import CompiledGraph
    from .mcp_client import MCPClient

    from .mcp_client import MCPClient

from .checkpointer import EncryptedSerializer
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver
from .tools.artifacts import create_artifact
import asyncio

load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# Define State
class AgentState(TypedDict):
    """The state of the agent graph."""

    messages: Annotated[list[BaseMessage], add_messages]
    intent: str  # "conversation" or "task"
    task: str  # Phase 6: Original user request for finalizer context
    plan: list[str]  # The high-level plan generated by the planner
    past_steps: list[tuple[str, str]]  # (step_description, result) for completed steps
    current_step: int  # Current index in the plan
    iteration_count: int  # To prevent infinite loops
    trace_id: str  # Phase 5.3: Unique ID for observability
    user_context: dict # Phase 3A: Injected user identity


class AgentEngine:
    """The core engine that runs agents via LangGraph."""

    llm: BaseChatModel | Runnable[object, AIMessage]
    history: list[BaseMessage]
    last_used: float

    def __init__(self, agent: Agent) -> None:
        self.agent = agent
        self.history = []
        self.cwd = os.getcwd()
        self.last_used = time.time()
        self.tools: list[dict[str, object]] = []
        self.mcp_clients: list["MCPClient"] = []
        self.tool_to_client: dict[str, "MCPClient"] = {}
        self.tool_to_client: dict[str, "MCPClient"] = {}
        self.tool_name_map: dict[str, str] = {}  # Resolves namespaced_name -> original_name
        self.local_tool_registry: dict[str, Runnable] = {} # Phase 4: Local tools
        
        # Phase 5.3: Observability
        from .observability import trace_logger
        self.tracer = trace_logger

        # Use agent-specific provider if set, otherwise use global env
        env_provider = os.getenv("AI_PROVIDER", "ollama") or "ollama"
        raw_provider = agent.ai_provider or env_provider
        provider = raw_provider.lower()

        logger.info(f"Initializing engine for agent: {agent.role} using provider: {provider}")

        # Basic LLM initialization
        llm: BaseChatModel
        if provider == "ollama":
            model_name = os.getenv("OLLAMA_MODEL", "llama3")
            logger.info(f"Using Ollama model: {model_name}")
            llm = ChatOllama(model=model_name, temperature=0.1)
        elif provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEY not found and provider is 'openai'")
            model_name = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
            logger.info(f"Using OpenAI model: {model_name}")
            llm = ChatOpenAI(model=model_name, api_key=api_key, temperature=0.2)  # type: ignore[arg-type]
        else:
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key:
                raise ValueError("GEMINI_API_KEY not found and provider is 'gemini'")
            logger.info("Using Gemini model: gemini-1.5-flash")
            llm = ChatGoogleGenerativeAI(
                model="gemini-1.5-flash", google_api_key=api_key, temperature=0.7
            )

        # Dual LLM Architecture (Phase 6)
        self.llm_text = llm  # For Router, Planner, Finalizer (NO TOOLS)
        self.llm_exec = llm  # For Executor (WITH TOOLS, bound later)
        self.llm = llm  # Backward compatibility

        # Persistence (Phase 3A)
        # We perform a lazy init in initialize().
        self.db_path = "data/phylactery_memory.db"
        self._checkpointer_cm = None 
        self.checkpointer = None
        
        # Async Lifecycle
        self._initialized = False
        self._init_lock = asyncio.Lock()
        
        # Graph will be built in initialize()
        self.graph = None 
        
        # Load Local Tools (Phase 4)
        local_tools = [create_artifact]
        for t in local_tools:
            self.local_tool_registry[t.name] = t
            # Append metadata for Prompt Helper & Binding
            self.tools.append({
                "name": t.name,
                "description": t.description,
                "parameters": t.args # Schema
            })

    async def initialize(self) -> None:
        """Async initialization of resources (DB, Graph, Tools). Idempotent."""
        if self._initialized:
            return

        async with self._init_lock:
            if self._initialized:
                return

            logger.info(f"ðŸ”Œ [ENGINE] Initializing async resources for {self.agent.role}...")
            
            # Ensure data dir exists
            os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
            
            # Initialize AsyncSqliteSaver
            self._checkpointer_cm = AsyncSqliteSaver.from_conn_string(self.db_path)
            self.checkpointer = await self._checkpointer_cm.__aenter__()
            
            # Initialize MCP tools
            if self.agent.mcp_servers:
                await self._init_mcp_tools(self.agent.mcp_servers)
            
            # Build Graph (now that checkpointer is ready)
            self.graph = self._build_graph()
            
            self._initialized = True
            logger.info(f"âœ… [ENGINE] Initialization complete for {self.agent.role}")

    async def aclose(self) -> None:
        """Async cleanup of resources. Idempotent."""
        if not self._initialized:
            return

        async with self._init_lock:
            if not self._initialized:
                return
            
            logger.info(f"ðŸ”Œ [ENGINE] Closing resources for {self.agent.role}...")
            
            # Close MCP Clients
            for name, client in self.tool_to_client.items():
                try:
                    await client.disconnect()
                except Exception as e:
                    logger.warning(f"Error closing MCP client {name}: {e}")
            
            # Close Checkpointer
            if self._checkpointer_cm:
                try:
                    await self._checkpointer_cm.__aexit__(None, None, None)
                except Exception as e:
                    logger.warning(f"Error closing checkpointer: {e}")
                finally:
                    self._checkpointer_cm = None
                    self.checkpointer = None
            
            self._initialized = False
            logger.info(f"ðŸ’¤ [ENGINE] Shutdown complete for {self.agent.role}")

    def _sanitize_history(self, messages: list[BaseMessage]) -> list[BaseMessage]:
        """
        Sanitize history to fix OpenAI 400 errors:
        1. Remove orphaned ToolMessages (no preceding AIMessage with tool_calls)
        2. Ensure AIMessage with tool_calls is followed by ToolMessage
        """
        clean_messages = []
        for i, msg in enumerate(messages):
            if isinstance(msg, ToolMessage):
                # Check if previous message exists and has tool_calls
                if i > 0 and isinstance(messages[i-1], AIMessage) and messages[i-1].tool_calls:
                    # Also check if tool_call_id matches (optional but good)
                    # For now just keep it if parent exists
                    clean_messages.append(msg)
                else:
                    logger.warning(f"ðŸ§¹ Removing orphaned ToolMessage: {msg.content[:50]}...")
            elif isinstance(msg, AIMessage) and msg.tool_calls:
                # Check if next message is ToolMessage (or if we are waiting for one)
                # If this is the LAST message, we might be waiting for tool execution, so keep it.
                # If not last, and next is NOT tool message, then it's an orphan call?
                # Actually, OpenAI requires tool_calls to be followed by tool messages.
                # If we have [AIMessage(calls), HumanMessage], that's invalid.
                
                # Simple logic for now: Keep AIMessage, but if we filter out its ToolMessage later, 
                # we might have an issue. Start with just filtering orphans.
                clean_messages.append(msg)
            else:
                clean_messages.append(msg)
        return clean_messages


    async def _init_mcp_tools(self, servers: list[str]) -> None:
        """Initialize MCP clients and fetch tools using centralized config."""
        from .mcp_config import get_server_config
        from .mcp_client import MCPClient

        for server_name in servers:
            try:
                config = get_server_config(server_name)
                client = MCPClient(config)
                
                await client.connect()
                server_tools = await client.list_tools()
                self.mcp_clients.append(client)
                
                for t in server_tools:
                    # Namespace tools to avoid collisions (server_name_tool_name)
                    original_name = t['name']
                    tool_name = f"{server_name}_{original_name}"
                    
                    tool_dict = cast(dict[str, object], t)
                    tool_dict["name"] = tool_name
                    self.tools.append(tool_dict)
                    
                    self.tool_to_client[tool_name] = client
                    self.tool_name_map[tool_name] = original_name
                    
                logger.info(f"âœ… Connected to MCP Server: {server_name} ({len(server_tools)} tools)")
            except Exception as e:
                logger.error(f"âŒ Failed to initialize MCP Server {server_name}: {e}")

        if self.tools and isinstance(self.llm_exec, BaseChatModel):
            logger.info(f"ðŸ› ï¸ Binding {len(self.tools)} tools to Executor LLM.")
            self.llm_exec = self.llm_exec.bind_tools(self.tools)

    async def _audit_system_health(self) -> str:
        """Fetch active issues from Sentry if connected."""
        sentry_client = next((c for name, c in self.tool_to_client.items() if "sentry" in name), None)
        if not sentry_client:
            return ""
            
        try:
            # Attempt to list unresolved issues
            logger.info("ðŸ” [AUDIT] Checking Sentry for active issues...")
            # Note: We use the namespaced tool name
            issues = await sentry_client.call_tool("sentry.list_issues", {"query": "is:unresolved"})
            if issues and "Error" not in str(issues):
                return f"\n\n### CRITICAL ERRORS (Sentry)\n{issues[:1000]}"
        except Exception as e:
            logger.warning(f"âš ï¸ Sentry audit failed: {e}")
        return ""

    def _build_graph(self) -> "CompiledGraph":
        workflow = StateGraph(AgentState)

        # --- NODE: ROUTER (Conversational Entry Point) ---
        async def router(state: AgentState) -> dict[str, object]:
            """Conversational LLM that decides: answer directly or delegate to planner."""
            logger.info("ðŸ§  [ROUTER] Starting intent classification...")
            messages = state["messages"]
            trace_id = state.get("trace_id", "unknown")
            user_context = state.get("user_context", {}) # Phase 3A
            
            last_human_msg = next((m.content for m in reversed(messages) if isinstance(m, HumanMessage)), "")
            
            self.tracer.log_event("router", "input", {"message": str(last_human_msg)})

            # RAG Retrieval for context
            from .memory import memory
            relevant_docs = await memory.retrieve_relevant(str(last_human_msg), k=2)
            
            skills_context = ""
            if relevant_docs:
                skills_context = "\n\n### AVAILABLE KNOWLEDGE\n"
                for doc in relevant_docs:
                    skills_context += f"- {doc.metadata.get('name')}: {doc.page_content[:200]}...\n"

            # Proactive Health Audit
            health_context = await self._audit_system_health()

            # Dynamic Context Injection (Phase 3A)
            user_identity = f"User: {user_context.get('name', 'Operator')} ({user_context.get('role', 'Unknown')})"

            router_prompt = (
                f"You are Phylactery, a conversational AI assistant for SkullRender.\n"
                f"Role: {self.agent.role}\n"
                f"Instructions: {self.agent.instructions}\n"
                f"Interact with: {user_identity}\n" 
                f"{skills_context}"
                f"{health_context}\n\n"
                "### YOUR TASK\n"
                "Analyze the user's message and classify the intent:\n\n"
                "**TASK** - If the user wants you to DO something (create, write, modify, delete, run, execute, build, generate, etc.)\n"
                "**CONVERSATION** - If the user is asking a question, wants information, or is just chatting\n\n"
                "### CRITICAL RULES\n"
                "- Action verbs (crear, hacer, escribir, generar, ejecutar, etc.) = TASK\n"
                "- Questions (cÃ³mo, quÃ©, por quÃ©, etc.) = CONVERSATION\n"
                "- If unsure, default to TASK (better to try than refuse)\n\n"
                "### RESPONSE FORMAT (STRICT JSON)\n"
                "Return ONLY this JSON structure:\n"
                "{\"intent\": \"task\", \"task_description\": \"what to do\"}\n"
                "OR\n"
                "{\"intent\": \"conversation\", \"response\": \"your answer\"}\n\n"
                "### EXAMPLES\n"
                "User: 'Â¿CÃ³mo hago un PR?' â†’ {\"intent\": \"conversation\", \"response\": \"Para hacer un PR...\"}\n"
                "User: 'Crea un archivo test.txt' â†’ {\"intent\": \"task\", \"task_description\": \"Create file test.txt\"}\n"
                "User: 'Haz una pÃ¡gina web' â†’ {\"intent\": \"task\", \"task_description\": \"Build a web page\"}\n"
                "User: 'Â¿QuÃ© es Phylactery?' â†’ {\"intent\": \"conversation\", \"response\": \"Phylactery es...\"}\n"
            )

            response = await self.llm_text.ainvoke([SystemMessage(content=router_prompt)] + messages[-5:])
            logger.info(f"ðŸ§  Router raw response: {response.content[:200]}...")
            
            try:
                content = str(response.content)
                import re
                json_match = re.search(r'(\{[\s\S]*?"intent"[\s\S]*?\})', content)
                if json_match:
                    data = json.loads(json_match.group(1))
                    intent = data.get("intent", "conversation")
                    logger.info(f"ðŸŽ¯ Router classified intent: {intent}")
                    
                    if intent == "conversation":
                        # Direct response
                        answer = data.get("response", content)
                        return {
                            "messages": [AIMessage(content=answer)],
                            "intent": "conversation",
                            "task": "",  # Phase 6
                            "plan": [],
                            "past_steps": [],
                            "current_step": 0,
                            "iteration_count": 0,
                            "trace_id": trace_id
                        }
                    else:
                        # Delegate to planner
                        task_desc = data.get("task_description", str(last_human_msg))
                        logger.info(f"ðŸš€ Delegating to Planner: {task_desc}")
                        self.tracer.log_event("router", "classification", {"intent": "task", "description": task_desc})
                        return {
                            "intent": "task",
                            "task": task_desc,  # Phase 6: Save original task
                            "messages": [HumanMessage(content=task_desc)],
                            "plan": [],
                            "past_steps": [],
                            "current_step": 0,
                            "iteration_count": 0,
                            "trace_id": trace_id
                        }
            except Exception as e:
                logger.error(f"Router failed to parse JSON: {e}")
            
            # Fallback: Check for action verbs, default to task if found
            action_verbs = ["crea", "crear", "haz", "hacer", "escribe", "genera", "ejecuta", "construye", "build", "create", "write", "make", "generate"]
            if any(verb in str(last_human_msg).lower() for verb in action_verbs):
                logger.warning(f"âš ï¸ Router fallback: Detected action verb, treating as TASK")
                return {
                    "intent": "task",
                    "task": str(last_human_msg),  # Phase 6
                    "messages": [HumanMessage(content=str(last_human_msg))],
                    "plan": [],
                    "past_steps": [],
                    "current_step": 0,
                    "iteration_count": 0
                }
            
            # Final fallback: treat as conversation
            logger.warning(f"âš ï¸ Router fallback: Treating as CONVERSATION")
            return {
                "messages": [AIMessage(content=str(response.content))],
                "intent": "conversation",
                "plan": [],
                "past_steps": [],
                "current_step": 0,
                "iteration_count": 0
            }

        # --- NODE: PLANNER ---
        async def planner(state: AgentState) -> dict[str, object]:
            """Generates a step-by-step plan for the user request."""
            logger.info("ðŸ“‹ [PLANNER] Generating execution plan...")
            messages = state["messages"]
            past_steps = state.get("past_steps", [])
            last_human_msg = next((m.content for m in reversed(messages) if isinstance(m, HumanMessage)), "")

            # Context from past steps
            past_context = ""
            if past_steps:
                past_context = "\n\n### COMPLETED STEPS\n"
                for step_desc, result in past_steps:
                    past_context += f"- {step_desc}: {result[:100]}...\n"

            planner_prompt = (
                f"You are the PLANNER for Phylactery.\n"
                f"Task: {last_human_msg}\n"
                f"{past_context}\n"
                "### YOUR JOB\n"
                "Break down the task into a sequence of atomic steps.\n"
                "Each step should be a single, clear action.\n\n"
                "### RESPONSE FORMAT\n"
                "Return JSON: {\"plan\": [\"step1\", \"step2\", ...]}\n"
                "Example: {\"plan\": [\"Create directory 'test'\", \"Write file 'hello.txt' with content 'Hello World'\", \"List directory contents\"]}"
            )

            # Pass actual user message for better context (Phase 6 Fix)
            user_msg = HumanMessage(content=last_human_msg)
            response = await self.llm_text.ainvoke([SystemMessage(content=planner_prompt), user_msg])
            
            try:
                content = str(response.content)
                import re
                json_match = re.search(r'(\{[\s\S]*?"plan"[\s\S]*?\]\})', content)
                if json_match:
                    data = json.loads(json_match.group(1))
                    self.tracer.log_event("planner", "plan_generated", {"plan": data["plan"]})
                    return {"plan": data["plan"], "current_step": 0}
            except Exception as e:
                logger.error(f"Planner failed to generate JSON: {e}")
            
            return {"plan": [str(last_human_msg)], "current_step": 0}

        # --- NODE: EXECUTOR (Agent) ---
        async def executor(state: AgentState) -> dict[str, object]:
            """Executes the current step of the plan."""
            messages = state["messages"]
            plan = state.get("plan", [])
            step_idx = state.get("current_step", 0)
            logger.info(f"ðŸ¦´ [EXECUTOR] Executing step {step_idx + 1}/{len(plan)}: {plan[step_idx] if step_idx < len(plan) else 'N/A'}")
            
            current_task = plan[step_idx] if step_idx < len(plan) else "Complete the request."

            # Mandatory Rules
            mandatory_rules = (
                "\n\n### MANDATORY EXECUTION RULES\n"
                "1. **DEFAULT PATH**: If no path is specified, use `phylactery-app/`.\n"
                "2. **.ENV PROTECTION**: READ-ONLY access to .env files. NEVER modify them.\n"
                f"3. **CURRENT TASK**: {current_task}\n"
                "4. **PRECISION**: Execute ONLY the current task. Don't wander."
            )

            system_prompt = SystemMessage(
                content=f"Role: Executor\nScope: {self.agent.role}\n{mandatory_rules}"
            )

            # Tools help for executor
            tools_help = ""
            if self.tools:
                tools_help = "\n\n### AVAILABLE TOOLS\n" + "\n".join([f"- {t['name']}: {t['description']}" for t in self.tools])
                system_prompt.content += tools_help

            # Phase 6 Fix: Detect if last message is ToolMessage (post-tool contract)
            is_post_tool = len(messages) > 0 and isinstance(messages[-1], ToolMessage)
            
            if is_post_tool:
                # Force summary instead of allowing more tool calls
                system_prompt.content += (
                    "\n\n### POST-TOOL INSTRUCTION\n"
                    "Tool output has been received above. DO NOT call more tools.\n"
                    "Summarize the result and explain what changed. Mark the step as complete."
                )
                # Phase 6 CRITICAL FIX: Use llm_text (no tools) to prevent loops
                response = await self.llm_text.ainvoke([system_prompt] + messages)
            else:
                # Normal execution with tools
                response = await self.llm_exec.ainvoke([system_prompt] + messages)
            
            logger.info(f"Executor responded for step {step_idx}")

            # Manual JSON Parsing for tools (some models need it)
            if isinstance(response, AIMessage) and not response.tool_calls:
                import re
                json_match = re.search(r'(\{[\s\S]*?"name"[\s\S]*?"parameters"[\s\S]*?\})', str(response.content))
                if json_match:
                    try:
                        data = json.loads(json_match.group(1).strip())
                        tool_call = {"name": data["name"], "args": data["parameters"], "id": f"call_{int(time.time())}", "type": "tool_call"}
                        response = AIMessage(content=response.content, tool_calls=[tool_call])
                    except: pass

            return {"messages": [response], "iteration_count": state.get("iteration_count", 0) + 1}

        # --- NODE: TOOLS ---
        async def call_tools(state: AgentState) -> dict[str, list[BaseMessage]]:
            """Execute tool calls from the last message."""
            logger.info("ðŸ”§ [TOOLS] Calling MCP tools...")
            last_message = state["messages"][-1]
            tool_messages: list[BaseMessage] = []

            if hasattr(last_message, "tool_calls") and last_message.tool_calls:
                for tool_call in last_message.tool_calls:
                    tool_name = tool_call["name"]
                    
                    # 1. Try MCP Clients
                    client = self.tool_to_client.get(tool_name)
                    
                    # 2. Try Local Tools
                    local_tool = self.local_tool_registry.get(tool_name)

                    if client:
                        try:
                            # Resolve back to original name for the server
                            original_name = self.tool_name_map.get(tool_name, tool_name)
                            result = await client.call_tool(original_name, tool_call["args"])
                            # Phase 6 Fix: Stringify non-string results
                            if not isinstance(result, str):
                                import json
                                result = json.dumps(result, ensure_ascii=False, indent=2)
                            tool_messages.append(ToolMessage(content=result, tool_call_id=tool_call["id"]))
                        except Exception as e:
                            err_msg = f"Error executing MCP tool {tool_name}: {e}"
                            logger.error(err_msg)
                            tool_messages.append(ToolMessage(content=err_msg, tool_call_id=tool_call["id"]))

                    elif local_tool:
                        try:
                            # Invoke Local Tool
                            logger.info(f"ðŸ”§ [TOOLS] Executing Local Tool: {tool_name}")
                            result = local_tool.invoke(tool_call["args"])
                            tool_messages.append(ToolMessage(content=str(result), tool_call_id=tool_call["id"]))
                        except Exception as e:
                            err_msg = f"Error executing Local Tool {tool_name}: {e}"
                            logger.error(err_msg)
                            tool_messages.append(ToolMessage(content=err_msg, tool_call_id=tool_call["id"]))
                    
                    else:
                        logger.error(f"âŒ Tool not found: {tool_name}")
                        tool_messages.append(ToolMessage(content=f"Error: Tool {tool_name} not found.", tool_call_id=tool_call["id"]))

            
            self.tracer.log_event("tools", "execution_results", {"count": len(tool_messages)})
            return {"messages": tool_messages}

        def route_from_router(state: AgentState) -> str:
            """Routes based on intent classification."""
            intent = state.get("intent", "conversation")
            if intent == "task":
                return "planner"
            return END

        def reviewer(state: AgentState) -> str:
            """Decides whether to continue the plan, finalize, or end."""
            last_message = state["messages"][-1]
            if hasattr(last_message, "tool_calls") and last_message.tool_calls:
                return "tools"
            
            step_idx = state.get("current_step", 0)
            plan = state.get("plan", [])
            iteration_count = state.get("iteration_count", 0)
            intent = state.get("intent", "conversation")
            
            # Phase 6 Fix: Route to finalizer for task completion
            if intent == "task":
                # If plan is complete or hit iteration limit, finalize
                if step_idx + 1 >= len(plan) or iteration_count >= 15:
                    return "finalizer"
                # Otherwise continue to next step
                if step_idx + 1 < len(plan):
                    return "next_step"
            
            return END

        # --- NODE: FINALIZER (Phase 6) ---
        async def finalizer(state: AgentState) -> dict[str, object]:
            """Synthesizes natural language response from execution results."""
            logger.info("âœ¨ [FINALIZER] Synthesizing final response...")
            past_steps = state.get("past_steps", [])
            task = state.get("task", "")  # Phase 6 Fix: Use original task
            iteration_count = state.get("iteration_count", 0)
            messages = state["messages"]
            
            # Build context from completed steps
            steps_summary = ""
            if past_steps:
                steps_summary = "\n### COMPLETED STEPS\n"
                for step_desc, result in past_steps:
                    steps_summary += f"- {step_desc}: {result[:150]}...\n"
            
            # Detect if stopped by safety limit
            stopped_early = iteration_count >= 15
            
            finalizer_prompt = (
                f"You are Phylactery's Finalizer. Your job is to translate execution results into natural language.\n\n"
                f"Original request: {task or 'Unknown'}\n"  # Phase 6 Fix
                f"{steps_summary}\n\n"
                f"### YOUR TASK\n"
            )
            
            if stopped_early:
                finalizer_prompt += (
                    "The execution was stopped due to safety limits (15 iterations).\n"
                    "Explain what was accomplished and what remains incomplete.\n"
                )
            else:
                finalizer_prompt += (
                    "Summarize what was accomplished in a friendly, conversational tone.\n"
                    "Be specific about files created, actions taken, and results.\n"
                )
            
            finalizer_prompt += (
                "\n### RESPONSE FORMAT\n"
                "Respond in natural language (Spanish or English). Be concise but informative.\n"
                "Example: 'He creado el archivo prueba5.txt en la carpeta phylactery-app con el contenido solicitado. Â¡Listo!'\n"
            )
            
            response = await self.llm_text.ainvoke([SystemMessage(content=finalizer_prompt)])
            logger.info(f"âœ¨ Finalizer generated response: {str(response.content)[:100]}...")
            
            return {
                "messages": [AIMessage(content=response.content)],
                "intent": "conversation",  # Mark as complete
                "iteration_count": iteration_count + 1
            }

        def advance_step(state: AgentState) -> dict[str, object]:
            """Advances to next step and records the completed step."""
            current_idx = state.get("current_step", 0)
            plan = state.get("plan", [])
            past_steps = state.get("past_steps", [])
            
            # Record completed step
            if current_idx < len(plan):
                last_msg = state["messages"][-1] if state["messages"] else None
                result = str(last_msg.content).strip() if last_msg else ""
                
                # Phase 6 Fix: Fallback for empty results
                if not result:
                    for m in reversed(state["messages"]):
                        if isinstance(m, ToolMessage) and m.content:
                            result = str(m.content)[:500]
                            break
                if not result:
                    result = "Completed"
                
                past_steps.append((plan[current_idx], result))
            
            return {
                "current_step": current_idx + 1,
                "past_steps": past_steps
            }

        workflow.add_node("router", router)
        workflow.add_node("planner", planner)
        workflow.add_node("executor", executor)
        workflow.add_node("tools", call_tools)
        workflow.add_node("advance", advance_step)
        workflow.add_node("finalizer", finalizer)  # Phase 6

        workflow.set_entry_point("router")
        
        workflow.add_conditional_edges(
            "router",
            route_from_router,
            {
                "planner": "planner",
                END: END
            }
        )
        
        workflow.add_edge("planner", "executor")
        
        workflow.add_conditional_edges(
            "executor",
            reviewer,
            {
                "tools": "tools",
                "next_step": "advance",
                "finalizer": "finalizer",  # Phase 6
                END: END
            }
        )
        workflow.add_edge("tools", "executor")
        workflow.add_edge("advance", "executor")
        workflow.add_edge("finalizer", END)  # Phase 6: Finalizer always ends

        workflow.add_edge("finalizer", END)  # Phase 6: Finalizer always ends

        return workflow.compile(checkpointer=self.checkpointer)

    async def _compress_history(self, messages: list[BaseMessage]) -> list[BaseMessage]:
        """
        Phase 5.3.2: Compresses long conversation history into a summary.
        Threshold: 10 messages.
        """
        if len(messages) <= 10:
            return messages

        logger.info(f"ðŸ’¾ [COMPRESSOR] History length ({len(messages)}) exceeds threshold. Summarizing...")
        
        # Phase 5.3 Audit Fix: Preserve initial system messages if any
        system_msgs = [m for m in messages if isinstance(m, SystemMessage)]
        other_msgs = [m for m in messages if not isinstance(m, SystemMessage)]
        
        # We summarize the portion between (system) and (last 3)
        to_summarize = other_msgs[:-3]
        latest_msgs = other_msgs[-3:]
        
        summary_prompt = (
            "Summarize the preceding conversation between the user and the assistant.\n"
            "Focus on key decisions made, files created, and data retrieved.\n"
            "Keep it clinical and concise (max 200 words).\n"
            "Include current task status if applicable."
        )
        
        try:
            summary_response = await self.llm_text.ainvoke(
                [SystemMessage(content=summary_prompt)] + to_summarize
            )
            summary_text = str(summary_response.content)
            
            # Create a compressed history: [Original Systems] + [Summary] + [Latest]
            compressed = system_msgs + [
                SystemMessage(content=f"PREVIOUS CONVERSATION SUMMARY:\n{summary_text}"),
                *latest_msgs
            ]
            
            logger.info("âœ… [COMPRESSOR] History compressed successfully.")
            self.tracer.log_event("engine", "compression", {"original_len": len(messages), "new_len": len(compressed)})
            return compressed
        except Exception as e:
            logger.error(f"âŒ [COMPRESSOR] Compression failed: {e}")
            return messages

    async def ainvoke(self, message: str, user_context: dict = {}) -> str:
        """Runs the graph with a single user message, maintaining history via Persistence."""
        # Hardening: Ensure initialized even if loader logic was bypassed
        await self.initialize()
        self.last_used = time.time()
        
        # Phase 5.3.1 Start Trace
        trace_id = f"trace_{uuid.uuid4().hex[:12]}"
        self.tracer.start_trace(trace_id, {"role": self.agent.role})
        
        # Determine Thread ID (Persistency Key)
        # user_id is best, falling back to 'default_thread'
        thread_id = user_context.get("user_id", "default_thread")
        config = {"configurable": {"thread_id": thread_id}}

        current_message = HumanMessage(content=message)
        
        # Check current state from DB (Async fix)
        current_state = await self.graph.aget_state(config)
        
        # If we have history, we assume it's valid. 
        # We do NOT append to self.history manually anymore (RAM cache deprecated for execution)
        # But we might need to compress if DB history is long.
        
        if current_state.values and "messages" in current_state.values:
            db_messages = current_state.values["messages"]
            # Check length for compression (Phase 5.3.2)
            if len(db_messages) > 10:
                # We need to run compression. But compression updates history.
                # Complex: We can't easily modify DB history in-place without running the graph.
                # For Phase 3A, let's just append the new message and let the graph handle it
                # OR, strictly for now, we leave compression off or implement a reducer later.
                pass

        # Inputs: ONLY the new message. The reducer will add it to history.
        inputs = {
            "messages": [current_message],
            "intent": "", # Let Router decide
            "plan": [],
            "past_steps": [],
            "current_step": 0,
            "iteration_count": 0,
            "trace_id": trace_id,
            "user_context": user_context
        }
        
        try:
            # Pass config with thread_id for persistence
            result = await self.graph.ainvoke(inputs, config=config)
            
            # Sync RAM history for fallback/other uses (legacy)
            self.history = result["messages"]

            ai_responses = [m for m in result["messages"] if isinstance(m, AIMessage)]
            
            # End Trace
            self.tracer.end_trace("success")
            
            if ai_responses:
                return str(ai_responses[-1].content)
        except Exception as e:
            logger.error(f"Graph execution failed: {e}")
            self.tracer.log_event("engine", "critical_error", {"error": str(e)})
            self.tracer.end_trace("failed")
            return f"The spirits are disturbed: {e}"

        return "The spirits are silent."
